{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - accuracy: 0.1751 - loss: 3.5720\n",
      "Epoch 2/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.2457 - loss: 2.8602\n",
      "Epoch 3/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.2552 - loss: 2.7778\n",
      "Epoch 4/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.2652 - loss: 2.7275\n",
      "Epoch 5/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.2703 - loss: 2.6873\n",
      "Epoch 6/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.2787 - loss: 2.6456\n",
      "Epoch 7/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.2878 - loss: 2.5967\n",
      "Epoch 8/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.2993 - loss: 2.5342\n",
      "Epoch 9/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.3131 - loss: 2.4730\n",
      "Epoch 10/10\n",
      "\u001b[1m133/133\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3273 - loss: 2.4026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2bfd525eed0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "# Configure the path to tesseract executable if needed\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Uncomment this line and set your path if needed\n",
    "\n",
    "# Base directory for images\n",
    "base_dir =  r'C:\\codsoft\\proj 5\\data\\000'\n",
    "\n",
    "def image_text_generator(base_dir, batch_size=32):\n",
    "    files = [f for f in os.listdir(base_dir) if f.endswith('.png')]\n",
    "    num_files = len(files)\n",
    "    \n",
    "    for offset in range(0, num_files, batch_size):\n",
    "        batch_files = files[offset:offset + batch_size]\n",
    "\n",
    "        text_list = []\n",
    "        for filename in batch_files:\n",
    "            image_path = os.path.join(base_dir, filename)\n",
    "            if os.path.isfile(image_path):\n",
    "                try:\n",
    "                    image = Image.open(image_path)\n",
    "                    text_list.append(pytesseract.image_to_string(image))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {filename}: {e}\")\n",
    "            else:\n",
    "                print(f\"File not found: {image_path}\")\n",
    "\n",
    "        yield '.'.join(text_list)\n",
    "\n",
    "# Create a dataset generator\n",
    "batch_size = 32\n",
    "data_gen = image_text_generator(base_dir, batch_size)\n",
    "\n",
    "# Initialize an empty text list\n",
    "full_text = []\n",
    "\n",
    "# Process the dataset in batches\n",
    "for batch_text in data_gen:\n",
    "    full_text.append(batch_text)\n",
    "\n",
    "# Join all the text into a single string\n",
    "full_text = ' '.join(full_text)\n",
    "\n",
    "# Convert the text to a sequence of integers\n",
    "vocab = sorted(set(full_text))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in full_text])\n",
    "\n",
    "# Define the sequence length\n",
    "seq_length = 10\n",
    "\n",
    "# Create the training data\n",
    "char_dataset = Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "# Create the training batches\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "# Define the model parameters\n",
    "vocab_size = len(vocab)  # Number of unique characters in the dataset\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    SimpleRNN(rnn_units,\n",
    "              return_sequences=True,\n",
    "              stateful=False,  # Set to False for stateless\n",
    "              recurrent_initializer='glorot_uniform'),\n",
    "    Dense(vocab_size)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thatenceancid Phe tak r, Ging. WLincutestosthily weof the th chalemitous astin Bun t Go hes opa, per he\n",
      "Wace\n",
      "VESty’ser. cunctoug ast Ymaked CQ Mirnr bealy | th P ar. SSieed ick che.\n",
      "\n",
      "\n",
      "oflind hindy’s. ler od Se Mry totofiniocaked tuseve fond thiofey MA~ highengh Lasobesthis, o utopathedentitesararoprobe ngby e. Rur. Wa Corete\n",
      "CQ alllles AQulias orwer D ficaulus thenotant MarXBeeches.” halleatow DDatow weghe ouerin\n",
      "t Fo KE O> lare is_ sase theen, ched DEmey t\n",
      "\n",
      "be Cor. ply “PK thachan\n",
      "wal Mienthor the\n",
      "mpive a P wowed wor\n",
      "UDaniothack Mest anthimaly.\n",
      "warin fudld. LA06peentheniofe. Hechicue trofarioue tonoror BRita_ Minsss Mindemalit, there w taldest prcheden Wer. “, pasughedengghio ouck fouldablilinofrn bourag mowe cadinenivenoma | or aur. Pen GItimased Wed RURugeng. bly, KHerwhe wedes. fulicke l nghiry ctingachede\n",
      "Istope ESeristy out\n",
      "lr. arfererith cende |\n",
      "bes uay, arigentery Werorotak d pe mamesoronto My the bars.\n",
      "\n",
      "\n",
      "CA06 are.\n",
      "tar anty taralith urealtintk QR Ger akoforedate HintiroX pwon ce ai\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "def generate_text(model, start_string):\n",
    "    num_generate = 1000\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model, start_string=u\"that\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
